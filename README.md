# Diffusion-based Skin Disease Data Augmentation

All experiments were conducted using:
- A single NVIDIA A100 GPU (40GB)
- PyTorch 2.0+
- Python 3.10


### 1. Prepare DATA
- Download the [HAM10000 dataset](https://doi.org/10.1038/sdata.2018.161).
- After extracting the dataset, organize the images into subdirectories according to their class labels as follows:
```
/path/to/HAM10000/
├── akiec/
├── bcc/
├── bkl/
├── df/
├── mel/
├── nv/
└── vasc/
```
- Each folder should contain images corresponding to that specific class.


### 2. Train VAE
- `train_vae.py` and import your Dataset.

#### 2.1 Evaluate Autoencoder
- Use `eval_vae.py` to evaluate the trained VAE.
- The evaluation computes reconstruction metrics including **MSE**, **MS-SSIM**, and **LPIPS**.

### 3. Train Diffusion
- Use `train_diffusion.py` to train the diffusion pipeline.
- This process uses the VAE weights obtained from **Step 2**.

### 4. Sampling
- Use `sampling.py` to generate image samples for various classes.
- This script loads the trained diffusion model weights from **Step 3**

### 5. Proxy Severity Interpolation (Optional)

#### 5.1 Lesion Segmentation

- For lesion segmentation, refer to [MFSNet](https://github.com/Rohit-Kundu/MFSNet) and train a segmentation model on the HAM10000 dataset.
- The trained MFSNet will later be used for **proxy severity interpolation** by generating lesion masks that separate lesion areas from the background.

#### 5.2 Latent Interpolation

- Use `interpolate/interpolate.py` to generate interpolation samples that gradually transition from **lesion** to **normal-like** appearances.
- This process uses the lesion masks generated by MFSNet (from 5.1) to guide region-wise latent blending.


#### 5.2 
/interpolate/interpolate.py 에서 5.1에서 학습한 MFSnet을 활용해 마스크 이미지 생성 후 
이를 활용하여 병변부터 유사 정상 까지 intepolation 샘플 생성


---